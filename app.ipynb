{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "6czvedaAozHQ",
        "HZdIqtIPo4Dh",
        "K8QagqWGo6nN"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## requeriments.txt"
      ],
      "metadata": {
        "id": "6czvedaAozHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# !pip install bitsandbytes==0.40.0\n",
        "# !pip install accelerate==0.21.0\n",
        "# !pip install langchain\n",
        "# !pip install langchain_community\n",
        "# !pip install PyPDF2\n",
        "# !pip install pypdf\n",
        "# !pip install pinecone\n",
        "# !pip install sentence-transformers\n",
        "# !pip install -U pinecone-client langchain\n",
        "# !pip install langchain_pinecone\n",
        "# !pip install gradio"
      ],
      "metadata": {
        "id": "sGzQdYsj48XK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pinecone\n",
        "import transformers\n",
        "import gradio as gr\n",
        "from pinecone import Pinecone\n",
        "from torch import cuda, bfloat16\n",
        "from langchain.vectorstores import Pinecone\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from transformers import StoppingCriteria, StoppingCriteriaList\n",
        "from langchain.chains import StuffDocumentsChain, LLMChain, ConversationalRetrievalChain"
      ],
      "metadata": {
        "id": "76KPpXVySflD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## model.py"
      ],
      "metadata": {
        "id": "HZdIqtIPo4Dh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=bfloat16\n",
        ")\n",
        "\n",
        "hf_auth = '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'\n",
        "model_config = transformers.AutoConfig.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        ")\n",
        "\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    config=model_config,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        "    use_auth_token=hf_auth\n",
        ")\n",
        "\n",
        "model.eval()\n",
        "\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        ")\n",
        "\n",
        "stop_list = ['\\nHuman:', '\\n```\\n']\n",
        "\n",
        "stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n",
        "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
        "\n",
        "class StopOnTokens(StoppingCriteria):\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
        "        for stop_ids in stop_token_ids:\n",
        "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "stopping_criteria = StoppingCriteriaList([StopOnTokens()])\n",
        "\n",
        "generate_text = transformers.pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=True,\n",
        "    task='text-generation',\n",
        "    stopping_criteria=stopping_criteria,\n",
        "    temperature=0.1,\n",
        "    max_new_tokens=512,\n",
        "    repetition_penalty=1.1\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=generate_text)"
      ],
      "metadata": {
        "id": "LVI5PpvvK5PC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## app.py"
      ],
      "metadata": {
        "id": "K8QagqWGo6nN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gui(path):\n",
        "  loader = PyPDFLoader(path)\n",
        "  documents = loader.load()\n",
        "\n",
        "  text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "  texts = text_splitter.split_documents(documents)\n",
        "\n",
        "  os.environ['PINECONE_API_KEY'] = \"73439b95-7aed-4b14-a18c-ad93cf4e9bef\"\n",
        "  embeddings = HuggingFaceEmbeddings()\n",
        "  index_name = \"hannah\"\n",
        "  docsearch = PineconeVectorStore.from_documents(texts, embeddings, index_name=index_name)\n",
        "\n",
        "  qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm,\n",
        "    docsearch.as_retriever(search_kwargs={'k': 2}),\n",
        "    return_source_documents=False,\n",
        "  )\n",
        "\n",
        "  chat_history = []\n",
        "\n",
        "  query = \"\"\"you're a helpful assistente\n",
        "          - Detail this file and sum my expenses:\n",
        "  \"\"\"\n",
        "\n",
        "  result = qa_chain({'question': query, 'chat_history': chat_history})\n",
        "  answer = result[\"answer\"]\n",
        "\n",
        "  clean_answer = answer.split(\"Helpful Answer:\")[-1].strip() if \"Helpful Answer:\" in answer else answer\n",
        "  return (clean_answer)\n",
        "\n",
        "demo = gr.Interface(\n",
        "    gui,\n",
        "    [\n",
        "        gr.UploadButton(\"Upload a file\"),\n",
        "    ],\n",
        "    \"text\",\n",
        ")\n",
        "\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "P1LQhDcrW_gP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}